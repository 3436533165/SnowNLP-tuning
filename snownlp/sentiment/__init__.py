# -*- coding: utf-8 -*-
from __future__ import unicode_literals

import os
import codecs
import sys
import time
import threading
import queue
import marshal
import gzip
from collections import deque
from functools import partial

from snownlp import normal
from snownlp import seg
from snownlp.classification.bayes import Bayes, AddOneProb

# å®šä¹‰SimpleFreqç±»æ›¿ä»£sentiment.freq
class SimpleFreq:
    def __init__(self):
        self.d = {}
        self.total = 0

# è‡ªå®šä¹‰Bayesåˆ†ç±»å™¨ï¼Œè§£å†³åŠ è½½é—®é¢˜
class CustomBayes(Bayes):
    def __init__(self):
        super().__init__()
        # ç¡®ä¿då­—å…¸åŒ…å«negå’Œpos
        self.d = {'neg': SimpleFreq(), 'pos': SimpleFreq()}
        self.total = {'neg': 0, 'pos': 0}
    
    def load(self, fname, iszip=True):
        """ä¼˜åŒ–åŠ è½½æ¨¡å‹æ–‡ä»¶ï¼šæ”¯æŒå¢é‡è®­ç»ƒ"""
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(fname):
                # æ£€æŸ¥æ˜¯å¦æœ‰å‹ç¼©ç‰ˆæœ¬
                if iszip:
                    gz_name = fname + '.gz'
                    if os.path.exists(gz_name):
                        fname = gz_name
                    else:
                        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {fname} æˆ– {gz_name}")
                else:
                    raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {fname}")
                
            print(f"âœ… åŠ è½½æ¨¡å‹ä¸­: {fname}")
            if fname.endswith('.gz'):
                with gzip.open(fname, 'rb') as f:
                    data = marshal.load(f)
            else:
                with open(fname, 'rb') as f:
                    data = marshal.load(f)
            
            # æ›´æ–°æ¨¡å‹æ•°æ®
            self.total = data.get('total', {'neg': 0, 'pos': 0})
            
            # ç¡®ä¿då­—å…¸åŒ…å«negå’Œpos
            self.d = {'neg': SimpleFreq(), 'pos': SimpleFreq()}
            
            # ä»åŠ è½½çš„æ•°æ®ä¸­æ¢å¤negå’Œpos
            if 'd' in data:
                for category in ['neg', 'pos']:
                    if category in data['d']:
                        self.d[category].d = data['d'][category].get('d', {})
                        self.d[category].total = data['d'][category].get('total', 0)
                        
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
            print(f"  è´Ÿé¢è®­ç»ƒæ•°æ®: {self.total['neg']:,} æ¡")
            print(f"  æ­£é¢è®­ç»ƒæ•°æ®: {self.total['pos']:,} æ¡")
        except Exception as e:
            print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            # åˆ›å»ºæ–°æ¨¡å‹ç»“æ„
            self.d = {'neg': SimpleFreq(), 'pos': SimpleFreq()}
            self.total = {'neg': 0, 'pos': 0}

class Sentiment(object):
    def __init__(self, load_path=None):
        self.classifier = CustomBayes()  # ä½¿ç”¨è‡ªå®šä¹‰Bayesåˆ†ç±»å™¨
        if load_path:
            self.load(load_path)

    def save(self, fname, iszip=True):
        self.classifier.save(fname, iszip)

    def load(self, fname, iszip=True):
        self.classifier.load(fname, iszip)

    def handle(self, doc):
        words = seg.seg(doc)
        words = normal.filter_stop(words)
        return words

    def train(self, neg_docs, pos_docs, verbose=True, num_workers=4):
        """å¤šçº¿ç¨‹è®­ç»ƒå®ç° - ä¼˜åŒ–ç‰ˆ"""
        if num_workers is None:
            try:
                num_workers = min(16, max(2, os.cpu_count() - 1))
            except:
                num_workers = 4
                
        data = []
        total = len(neg_docs) + len(pos_docs)
        
        if verbose:
            print("\n" + "="*60)
            print(f"ğŸš€ å¼€å§‹è®­ç»ƒï¼Œæ€»å…± {total:,} æ¡æ ·æœ¬")
            print(f"  æ–°å¢è´Ÿé¢æ ·æœ¬: {len(neg_docs):,} æ¡")
            print(f"  æ–°å¢æ­£é¢æ ·æœ¬: {len(pos_docs):,} æ¡")
            if hasattr(self.classifier, 'total'):
                print(f"  å†å²è´Ÿé¢è¯æ¡: {self.classifier.total['neg']:,}")
                print(f"  å†å²æ­£é¢è¯æ¡: {self.classifier.total['pos']:,}")
            print(f"  ä½¿ç”¨ {num_workers} ä¸ªå·¥ä½œçº¿ç¨‹")
            print("="*60 + "\n")
        
        start_time = time.time()
        last_update_time = start_time
        last_processed = 0
        progress_lock = threading.Lock()
        
        # ä½¿ç”¨çº¿ç¨‹å®‰å…¨çš„é˜Ÿåˆ—
        job_queue = queue.Queue()
        processed_count = 0
        processed_lock = threading.Lock()
        
        # å¤šçº¿ç¨‹å¤„ç†å‡½æ•°
        def process_doc(doc, doc_type):
            words = self.handle(doc)
            label = 'neg' if doc_type == 'neg' else 'pos'
            return [words, label]
        
        # æ·»åŠ æ‰€æœ‰ä»»åŠ¡åˆ°é˜Ÿåˆ—
        for doc in neg_docs:
            job_queue.put((doc, 'neg'))
        for doc in pos_docs:
            job_queue.put((doc, 'pos'))
        
        # è¿›åº¦ç›‘æ§çº¿ç¨‹
        def progress_monitor():
            nonlocal last_update_time, last_processed, processed_count
            start = time.time()
            last_count = 0
            speed_history = deque(maxlen=10)
            
            if verbose:
                self._print_progress(0, "åˆå§‹åŒ–...", 0, 0, 0)
            
            while processed_count < total:
                time.sleep(0.2)
                
                with processed_lock:
                    current_count = processed_count
                
                if current_count > last_count:
                    current_time = time.time()
                    items_processed = current_count - last_count
                    time_elapsed = current_time - last_update_time
                    current_speed = items_processed / time_elapsed if time_elapsed > 0 else 0
                    
                    speed_history.append(current_speed)
                    avg_speed = sum(speed_history) / len(speed_history) if speed_history else 0
                    
                    percent = (current_count / total) * 100
                    elapsed = current_time - start_time
                    remaining_items = total - current_count
                    estimated_remain = remaining_items / avg_speed if avg_speed > 0 else 0
                    status = f"å¤„ç†ä¸­: {current_count:,}/{total:,}"
                    
                    if verbose:
                        with progress_lock:
                            self._print_progress(
                                percent, 
                                status, 
                                elapsed, 
                                estimated_remain,
                                avg_speed
                            )
                    
                    last_update_time = current_time
                    last_count = current_count
                    
            # æœ€ç»ˆæ˜¾ç¤º100%è¿›åº¦
            if verbose:
                with progress_lock:
                    elapsed = time.time() - start_time
                    self._print_progress(100.0, "å¤„ç†å®Œæˆ", elapsed, 0, avg_speed)
                    sys.stdout.write("\n")
                    sys.stdout.flush()
        
        # å·¥ä½œçº¿ç¨‹å‡½æ•°
        def worker():
            nonlocal data, processed_count
            batch_size = 50
            batch_data = []
            local_count = 0
            
            while True:
                try:
                    doc, doc_type = job_queue.get(timeout=0.1)
                    result = process_doc(doc, doc_type)
                    batch_data.append(result)
                    local_count += 1
                    
                    if local_count >= batch_size:
                        with processed_lock:
                            data.extend(batch_data)
                            processed_count += local_count
                        batch_data = []
                        local_count = 0
                    
                    job_queue.task_done()
                except queue.Empty:
                    if local_count > 0:
                        with processed_lock:
                            data.extend(batch_data)
                            processed_count += local_count
                    break
        
        # åˆ›å»ºè¿›åº¦ç›‘æ§çº¿ç¨‹
        monitor_thread = threading.Thread(target=progress_monitor)
        monitor_thread.daemon = True
        monitor_thread.start()
        
        # åˆ›å»ºçº¿ç¨‹æ± 
        threads = []
        for _ in range(num_workers):
            t = threading.Thread(target=worker)
            t.daemon = True
            t.start()
            threads.append(t)
        
        # ç­‰å¾…æ‰€æœ‰å·¥ä½œçº¿ç¨‹å®Œæˆ
        for t in threads:
            t.join()
        
        if verbose:
            elapsed = time.time() - start_time
            print(f"\nâœ… æ ·æœ¬å¤„ç†å®Œæˆ! è€—æ—¶: {elapsed:.2f}ç§’")
            print(f"è®­ç»ƒåˆ†ç±»å™¨ä¸­...")
        
        # å®é™…è®­ç»ƒåˆ†ç±»å™¨
        train_start = time.time()
        self.classifier.train(data)
        
        # æ›´æ–°ç»Ÿè®¡
        if hasattr(self.classifier, 'total'):
            self.classifier.total['neg'] += len(neg_docs)
            self.classifier.total['pos'] += len(pos_docs)
        
        if verbose:
            elapsed_train = time.time() - train_start
            elapsed_total = time.time() - start_time
            
            print("\n" + "="*60)
            print(f"ğŸ‰ è®­ç»ƒå®Œæˆ!".center(60))
            print("-"*60)
            print(f"â±ï¸ æ€»è€—æ—¶: {elapsed_total:.2f}ç§’")
            print(f"  æ ·æœ¬å¤„ç†è€—æ—¶: {elapsed:.2f}ç§’")
            print(f"  æ¨¡å‹è®­ç»ƒè€—æ—¶: {elapsed_train:.2f}ç§’")
            print("-"*60)
            if hasattr(self.classifier, 'total'):
                print(f"ğŸ“Š ç´¯è®¡æ•°æ®ç»Ÿè®¡:")
                print(f"  è´Ÿé¢è®­ç»ƒæ•°æ®: {self.classifier.total['neg']:,} æ¡")
                print(f"  æ­£é¢è®­ç»ƒæ•°æ®: {self.classifier.total['pos']:,} æ¡")
            print("="*60)

    def _print_progress(self, percent, status, elapsed, remaining, speed):
        """æ˜¾ç¤ºè¿›åº¦æ¡"""
        bar_len = 40
        filled_len = int(bar_len * percent / 100)
        bar = 'â–ˆ' * filled_len + '-' * (bar_len - filled_len)
        
        elapsed_str = self.format_time(elapsed)
        remain_str = self.format_time(remaining)
        speed_str = f"{speed:.1f}æ¡/ç§’" if speed > 0 else ""
        
        progress_str = f'\r[{bar}] {percent:.1f}% - {status} - è€—æ—¶: {elapsed_str} - å‰©ä½™: {remain_str} - é€Ÿåº¦: {speed_str}'
        
        max_length = 100
        if len(progress_str) < max_length:
            progress_str += ' ' * (max_length - len(progress_str))
        else:
            progress_str = progress_str[:max_length]
        
        sys.stdout.write(progress_str)
        sys.stdout.flush()
    
    def format_time(self, seconds):
        """æ—¶é—´æ ¼å¼åŒ–è¾…åŠ©å‡½æ•°"""
        if seconds < 60:
            return f"{seconds:.1f}ç§’"
        elif seconds < 3600:
            minutes = seconds / 60
            return f"{minutes:.1f}åˆ†é’Ÿ"
        else:
            hours = seconds / 3600
            return f"{hours:.1f}å°æ—¶"
    
    def classify(self, sent):
        words = self.handle(sent)
        ret, prob = self.classifier.classify(words)
        if ret == 'pos':
            return prob
        return 1-prob


# å¯¼å‡ºç›¸å…³å‡½æ•°æ¥å£
def load_sentiment(fname, iszip=True):
    """åŠ è½½æƒ…æ„Ÿåˆ†ææ¨¡å‹"""
    global classifier
    classifier = Sentiment(load_path=fname)

def train_sentiment(neg_file, pos_file, model_file=None, verbose=True, num_workers=8):
    """è®­ç»ƒæƒ…æ„Ÿåˆ†ææ¨¡å‹ï¼Œæ”¯æŒå¢é‡è®­ç»ƒ"""
    global classifier
    
    # åˆå§‹åŒ–æ¨¡å‹å®ä¾‹
    if model_file:
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if os.path.exists(model_file):
            print(f"âœ… åŠ è½½ç°æœ‰æ¨¡å‹: {model_file}")
            classifier = Sentiment(load_path=model_file)
        else:
            print(f"âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°æ¨¡å‹: {model_file}")
            classifier = Sentiment()
    else:
        # ä½¿ç”¨ç°æœ‰å…¨å±€åˆ†ç±»å™¨
        if 'classifier' not in globals():
            classifier = Sentiment()
    
    # è¯»å–æ•°æ®
    try:
        print(f"\nğŸ“‚ è¯»å–è´Ÿé¢æ•°æ®æ–‡ä»¶: {neg_file}")
        with codecs.open(neg_file, 'r', 'utf-8', errors='replace') as f:
            neg = [line.rstrip("\r\n") for line in f]
        
        print(f"ğŸ“‚ è¯»å–æ­£é¢æ•°æ®æ–‡ä»¶: {pos_file}")
        with codecs.open(pos_file, 'r', 'utf-8', errors='replace') as f:
            pos = [line.rstrip("\r\n") for line in f]
    except Exception as e:
        print(f"âŒ è¯»å–æ•°æ®å¤±è´¥: {e}")
        return
    
    if verbose:
        print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
        print(f"  æ–°å¢è´Ÿé¢æ ·æœ¬: {len(neg):,}")
        print(f"  æ–°å¢æ­£é¢æ ·æœ¬: {len(pos):,}")
        if hasattr(classifier.classifier, 'total'):
            print(f"  å†å²è´Ÿé¢è¯æ¡: {classifier.classifier.total['neg']:,}")
            print(f"  å†å²æ­£é¢è¯æ¡: {classifier.classifier.total['pos']:,}")
        print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    
    # è®­ç»ƒæ¨¡å‹
    classifier.train(neg, pos, verbose, num_workers)
    
    # ä¿å­˜æ¨¡å‹
    if model_file:
        try:
            print(f"\nğŸ’¾ ä¿å­˜æ¨¡å‹åˆ°: {model_file}")
            classifier.save(model_file)
            print(f"âœ… æ¨¡å‹ä¿å­˜æˆåŠŸ!")
        except Exception as e:
            print(f"âŒ ä¿å­˜æ¨¡å‹å¤±è´¥: {e}")
    
    print("\nâœ… è®­ç»ƒå®Œæˆ!")

def save_sentiment(fname, iszip=True):
    """ä¿å­˜æƒ…æ„Ÿåˆ†ææ¨¡å‹"""
    global classifier
    if 'classifier' in globals():
        classifier.save(fname, iszip)
    else:
        print("âš ï¸ æ²¡æœ‰å¯ä¿å­˜çš„æ¨¡å‹")

def classify_sentiment(sent):
    """æƒ…æ„Ÿåˆ†ç±»æ¥å£"""
    global classifier
    if 'classifier' in globals():
        return classifier.classify(sent)
    else:
        print("âš ï¸ æƒ…æ„Ÿæ¨¡å‹æœªåŠ è½½")
        return 0.5

# åˆå§‹åŒ–å…¨å±€æ¨¡å‹
classifier = Sentiment()