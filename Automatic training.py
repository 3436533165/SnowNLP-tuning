import re
import pandas as pd
from snownlp import SnowNLP
from tqdm import tqdm

def analyze_novel_sentiment(input_file, output_file):
    # 1. æ–‡ä»¶è¯»å–ä¸éªŒè¯
    try:
        with open(input_file, 'r', encoding='utf-8') as f:
            content = f.read().strip()
        if not content:
            print(f"âŒ é”™è¯¯ï¼šæ–‡ä»¶ '{input_file}' ä¸ºç©ºï¼")
            return
    except Exception as e:
        print(f"âŒ æ–‡ä»¶è¯»å–å¤±è´¥ï¼š{str(e)}")
        return

    # 2. å¼ºåŒ–åˆ†å¥é€»è¾‘
    sentences = []
    current = ""
    splitters = {'ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼Œ', 'ï¼›', 'â€¦', '~', '\n', 'â€', 'ã€', 'ã€‹', 'ï¼ï¼Ÿ', 'ï¼Ÿï¼'}
    quote_starts = {'â€œ', 'ã€Š', 'ã€Œ'}
    
    for char in content:
        if char in splitters and current.strip():
            current += char
            if any(q in current for q in quote_starts) and not current.count('â€') % 2 == 0:
                continue
            if re.search(r'[é“å–Šé—®è¯´]ï¼š$', current[-3:]):
                continue
            sentences.append(current)
            current = ""
        else:
            current += char
    
    if current.strip():
        sentences.append(current)

    # 3. è¿‡æ»¤æ— æ•ˆå¥å­
    valid_sentences = []
    for sent in sentences:
        clean_sent = sent.strip()
        if len(clean_sent) > 2 and not all(char in splitters for char in clean_sent):
            valid_sentences.append(clean_sent)
    
    if not valid_sentences:
        print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆå¥å­ï¼è¯·æ£€æŸ¥æ–‡ä»¶å†…å®¹")
        return
    print(f"âœ… æœ‰æ•ˆå¥å­æ•°ï¼š{len(valid_sentences)}")

    # 4. æƒ…æ„Ÿåˆ†æå¤„ç†
    results = []
    print("â³ æƒ…æ„Ÿåˆ†æä¸­...")
    for sent in tqdm(valid_sentences, desc="è¿›åº¦"):
        try:
            if sent.startswith(('â€', 'ã€')) and results:
                prev_sent = results[-1].split(",", 1)[1]
                if prev_sent.endswith(('ï¼šâ€œ', 'ï¼šã€Œ')) or re.search(r'[é“å–Šé—®è¯´]ï¼š$', prev_sent[-3:]):
                    sent = prev_sent + sent
                    results.pop()
            
            s = SnowNLP(sent)
            label = 1 if s.sentiments >= 0.5 else 0
            results.append(f"{label},{sent}")
        except:
            results.append(f"0,{sent}")

    # 5. ç»“æœåå¤„ç†ï¼ˆæ­£åˆ™æ›¿æ¢ï¼‰
    content_str = "\n".join(results)
    
    # è§„åˆ™1: åˆå¹¶è¿ç»­æ¢è¡Œç¬¦
    content_str = re.sub(r'\n\n+', '\n', content_str)
    
    # è§„åˆ™2: è¡Œé¦–æ— æ ‡ç­¾è¡Œæ·»åŠ é»˜è®¤æ ‡ç­¾
    content_str = re.sub(r'(^|\n)(?!0,|1,)', r'\g<1>1,', content_str)
    
    # è§„åˆ™3: åˆ é™¤ä¸­æ–‡å¼•å·
    content_str = re.sub(r'[â€œâ€]', '', content_str)

    # 6. å†™å…¥æœ€ç»ˆç»“æœ
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(content_str)
        print(f"âœ… åˆ†æå®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³ï¼š{output_file}")
        
        # ç»“æœé¢„è§ˆ
        print("\nğŸ” ç»“æœæ ·ä¾‹ï¼š")
        for res in content_str.split('\n')[:3]:
            print(res)
            
    except Exception as e:
        print(f"âŒ ç»“æœå†™å…¥å¤±è´¥ï¼š{str(e)}")

if __name__ == "__main__":
    analyze_novel_sentiment("data.txt", "res.txt")